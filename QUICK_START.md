# è®­ç»ƒè„šæœ¬å¿«é€Ÿä½¿ç”¨æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1: ä¸€é”®å®Œæ•´æµæ°´çº¿ï¼ˆæ¨èæ–°æ‰‹ï¼‰
```bash
# ä»åŸå§‹æ–‡æœ¬æ–‡ä»¶å¼€å§‹ï¼Œå®Œæˆæ•´ä¸ªæµæ°´çº¿
./prepare_and_train.sh corpus.txt small
```

### æ–¹æ³•2: åˆ†æ­¥æ‰§è¡Œï¼ˆæ¨èè¿›é˜¶ç”¨æˆ·ï¼‰
```bash
# 1. è®­ç»ƒtokenizer
python -m cs336_basics.train_tokenizer \
    --input_path corpus.txt \
    --vocab_size 20000 \
    --output_path data/tokenizer.vocab

# 2. è½¬æ¢æ•°æ®ä¸ºtokens
python -m cs336_basics.data2token \
    --data_path corpus.txt \
    --vocab_path data/tokenizer.vocab \
    --merges_path data/tokenizer.merges \
    --output_path data/train_data.npy \
    --method streaming

# 3. å¼€å§‹è®­ç»ƒ
./run_training.sh small ./data ./experiments
```

## ğŸ“‹ å¯ç”¨é…ç½®

| é…ç½® | æ¨¡å‹å¤§å° | å‚æ•°é‡ | æ¨èç”¨é€” |
|------|----------|--------|----------|
| `debug` | æå° | ~0.5M | å¿«é€Ÿæµ‹è¯•ã€è°ƒè¯• |
| `small` | å° | ~50M | å®éªŒã€å­¦ä¹  |
| `medium` | ä¸­ç­‰ | ~117M | å°è§„æ¨¡åº”ç”¨ |
| `large` | å¤§ | ~350M | ç”Ÿäº§ç¯å¢ƒ |

## ğŸ”„ æ¢å¤è®­ç»ƒ

```bash
# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
./resume_training.sh ./experiments/checkpoints/checkpoint_50000.pt 100000
```

## ğŸ“Š ç›‘æ§è®­ç»ƒ

è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ `./experiments/logs/` ç›®å½•ä¸­ï¼ŒåŒ…å«ï¼š
- è®­ç»ƒæŸå¤±
- éªŒè¯æŸå¤±ï¼ˆå¦‚æœæœ‰éªŒè¯æ•°æ®ï¼‰
- è®­ç»ƒé€Ÿåº¦
- æ£€æŸ¥ç‚¹ä¿å­˜ä¿¡æ¯

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å†…å­˜ä¸è¶³
```bash
# ä½¿ç”¨æ›´å°çš„é…ç½®
./run_training.sh debug

# æˆ–è€…è‡ªå®šä¹‰å°æ‰¹æ¬¡å¤§å°ï¼ˆç¼–è¾‘è„šæœ¬ä¸­çš„BATCH_SIZEï¼‰
```

### è®­ç»ƒå¤ªæ…¢
```bash
# æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
python -c "import torch; print(torch.cuda.is_available())"

# ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
```

### æ•°æ®é—®é¢˜
```bash
# æ£€æŸ¥tokenizeræ–‡ä»¶
ls -la data/tokenizer.*

# æ£€æŸ¥tokenæ•°æ®
python -c "import numpy as np; print(np.load('data/train_data.npy').shape)"
```

## ğŸ“ æ–‡ä»¶ç»“æ„

è®­ç»ƒå®Œæˆåçš„ç›®å½•ç»“æ„ï¼š
```
./
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ tokenizer.vocab      # è¯æ±‡è¡¨
â”‚   â”œâ”€â”€ tokenizer.merges     # åˆå¹¶è§„åˆ™
â”‚   â”œâ”€â”€ train_data.npy       # è®­ç»ƒæ•°æ®tokens
â”‚   â””â”€â”€ val_data.npy         # éªŒè¯æ•°æ®tokens
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ checkpoints/         # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ logs/               # è®­ç»ƒæ—¥å¿—
â””â”€â”€ run_training.sh         # è®­ç»ƒè„šæœ¬
```

## ğŸ¯ ä¸‹ä¸€æ­¥

è®­ç»ƒå®Œæˆåï¼Œä½ å¯ä»¥ï¼š
1. ä½¿ç”¨æ£€æŸ¥ç‚¹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
2. åœ¨æ–°æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹
3. è¯„ä¼°æ¨¡å‹æ€§èƒ½
4. éƒ¨ç½²æ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒ
